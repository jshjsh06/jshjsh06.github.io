---
layout: post
title:  머신러닝 기본 요약(책 : 파이썬 라이브러리를 활용한 머신러닝)
date:   2021-02-02 23:47:00
image:  introduce_to_machine_learning_with_python.jpg
tags:   [머신러닝, 파이썬_라이브러리를_활용한_머신러닝]
---





# 머신러닝 기본 정리

## 1. 기타

- 과적합 줄이는 방법
  - 정규화 규제 사용하기
  - 데이터셋 늘리기
  - Feature를 줄여보기
  - Cross Validation
- 정규화(Regularization)란?
  - Norm : 백터의 길이 또는 크기를 측정하는 방법(함수)
  - 차수가 1이면 L1, 2면 L2 Norm
  - 즉, 변수의 계수에 벌점을 주어 모델을 좀더 general하게 만들어줍니다.
- R^2(결정계수)
  - 회귀 모델에서 예측 적합도를 0과 1사이로 계산한 것
  - 1 - 목표값과 예측값의 차이의 합으로 구하는 것
- RMSE, MAPE
  - rmse : 표준편차로 모델의 예측값과 실제 값의 차이를 숫자로 표현됨
  - mape : 예측값의 변화량.
  - 표준편차 : 분산의 제곱근
  - 분산 : 편차의 제곱의 합
  - 편차 : 각 데이터 값을 데이터 값들의 평균으로 뺀 것

## 2. 모델 기반 - 지도학습

### (1) KNN 최근접

- KNN(최근접) 이웃
  - 데이터 포인트에 대해 가장 가까운 데이터포인트를 찾는 방법
- KNN Regression
  - 이웃간의 거리의 평균으로 값을 구함
- 이해하기는 쉽지만 예측이 느리고 많은 특성에는 적합하지 않음. 그래서 잘 안씀



### (2) 선형모델

- 장점과 단점
  - 장점 : 일반적인 예측 모델로 분류모델과 달리 일어나지 않은 일을 예측 가능함. 비교적 많은 특성 사용 가능
  - 단점 : 데이터 전처리 방식에 따라 예측값이 달라짐. 

#### 1) 일반 회귀

- 선형 회귀(Linear Regression) == 최소제곱법
  - y값과 예측값 사이의 '평균제곱오차'를 최소화하는 방법
  - 1차원에서는 과소적합, 100개 이상의 차원에서는 과대적합의 위험성이 있음
- 릿지 회귀
  - L2 규제를 사용
  - 모든 특성의 영향을 최소한으로 만듬
- 라쏘 회귀
  - L1 규제를 사용
  - 일부 계수를 0으로 만듬. 특정 계수는 0이 될 수 있음.

#### 2) 분류용 선형모델

- 분류용 선형모델에서는 '결정 경계'가 입력의 선형 함수. 즉, 선, 평면, 초평면을 사용해서 클래스를 분류함

- 이진분류(binary classification)
  - 모델이라기 보단 방식을 의미함. binary classification을 이루기 위해 logistic regression, svm(서포트 벡터 머신) 모델등을 사용
  - 선형 회귀식과 아주 비슷. 대신 특성의 계수들을 그냥 사용하지 않고 예측한 값을 임계치 0과 비교
  - 0보다 값이 작으면 -1, 0보다 크면 1로 분류

- logistic regression
  - Linear Regression에서 odds(승산)을 사용해서 결정경계를 만드는 것
- Linear SVC
  - SVM 서포트 벡터 머신이라 보면 됨

### (3) 나이브 베이즈 분류기

- 베이즈 정리에 기반한 통계적 분류 기법

- Logistic Regression, LinearSVC 같은 선형 분류기보다 훈련 속도는 빠르지만 일반화 성능은 조금 떨어짐
- 각 특성을 개별로 취급해 파라미터를 학습. 각 특성에서 클래스별 통계를 단순하게 취합

### (4) 결정 트리

- 분류와 회귀 둘다 사용. 기본적인 결정 트리는 결정에 다다르기 위해 예/아니오 질문을 통해 학습
- 과대적합 막는 전략
  - 사전 가지치기 : 트리의 최대 깊이 및 리프 최대 갯수 제한
  - 사후 가지치기 : 트리를 만든 후 데이터 포인트가 적은 노드를 삭제하거나 병합

- 회귀에 사용할 때
  - 각 노드의 테스트 결과에 따라 트리를 탐색해나가고 새로운 데이터 포인트에 해당되는 리프 노드를 찾는다.
  - 찾는 리프 노드의 훈련 데이터 평균값이 출력된다.
  - 단! 훈련 데이터 밖의 새로운 데이터를 예측할 능력은 없음
- Decision Tree
  - 정말 단순하게 위에 적은 것처럼 '결정 트리는 결정에 다다르기 위해 예/아니오 질문을 통해 학습'
  - 각 노드의 테스트 결과에 따라 트리를 탐색해나가고 새로운 데이터 포인트에 해당되는 리프 노드를 찾는다.
- 장점과 단점
  - 장점 : 모델을 쉽게 시각화할 수 있어서 비전문가도 이해하기 쉬움. 전처리 과정도 많이 필요 없음
  - 단점 : 과대적합 경향 심함

### (5) 앙상블 결정트리

- 랜덤 포레스트
- Gradient Boosting

#### 1) 랜덤 포레스트

- 주요 단점인 과대적합을 피할수 있는 방법으로 기본적으로 조금씩 다른 결정 트리들의 묶음임.
- 서로 다른 방향으로 과대적합된 트리를 많이 만들면 그 결과를 평균냄으로써 과적합을 피함
- 이를 위해 결정 트리를 많이 만들고, 각 트리들은 구별되어야 함
  - 이를 위해 트리들이 달라지도록 트리 생성 시 무작위성을 주입함
- 부트스트랩 샘플을 사용함
  - 데이터포인트를 중복으로 여러번 반복 추출함
  - 위 추출한 데이터셋을 기준으로 결정 트리를 만듬
- 장단점
  - 장점 : 단일트리의 단점인 과대적합을 피할 수 있음
  - 단점 : 비전문가에게 보여주기 힘듬. 즉 이해가 어려움. 매우 차원이 높은 데이터에서는 잘 작동안함

#### 2) Gradeint Boosting

- 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 만드는 방법.
- 랜덤 포레스트와는 다르게 무작위성(부트스트랩 샘플)이 없는 대신 강력한 사전 가지치기를 사용
- 보통 5보타 깊은 트리를 사용하지 않음. 즉, 약한, 얕은 학습기를 사용
- 장단점
  - 장점 : 지도 학습에서 가장 강력하고 널리 사용되는 모델. 예측률이 좋고 안정적임.
  - 단점 : 훈련시간이 길고, 매개변수를 잘 조정해야함. 트리기반 특성상 고차원 데이터는 잘 안됨

### (6) SVM 커널 서포트 벡터 머신

- 결정경계, 즉 분류를 위한 기준 선을 정의하는 모델로. 데이터 사이 경계가 가장 큰 폭을 가진 경계를 찾는 알고리즘
  - 즉 결정 경계가 가장 큰 폭을 기준으로 하는 == 마진을 최대화하는 곳을 찾는 것
- 특히 2차원에서는 1개의 직선으로 구분이 안되는 경우, 다항식 커널을 사용하면 2차원 값을 3차원으로 표현하면 하나의 평면으로 그을 수 있다!
  - kernel을 'poly'로 하면 됨
- kenel을 'rbf'로 하면 데이터 포인트를 아주 '고차원' 데이터로 변환한다. 그래서 유연한 선을 만들 수 있게 한다.
- 장단점
  - 장점 : 다양한 데이터셋에서 잘 작동함. 특성이 적어도 복잡한 결정 경계 가능
  - 단점 : 데이터 전처리가 필요함, 매개변수 설정에 신경 많이 씀. '분석 및 설명이 난해함'. 데이터 스케일에 민감함

### (7) 딥러닝

- 퍼셉트론 -> 실제 뉴런을 통한 신경을 의미함
- 입력값을 몇개의 은닉층을 만들고 각 은닉층에는 은닉유닛을 만들어서 실제값과 비교하며 학습한다.
- 실제값과 예측값의 오차를 '경사 하강법'을 통해서 (오차역전파) 가중치를 조정한다.
- 장단점 
  - 장점 : 대량의 데이터에 내재된 정보를 잡아낸다. 많은 변수를 사용한 복잡한 모델 사용 가능
  - 단점 : 학습이 오래 걸린다. 데이터 스케일에 민감함

- 머신러닝과 비교했을 때 장단점
  - 머신러닝 : 적은 피쳐에 더 잘 작동하고, 딥러닝보다 설명력이 다소 더 높으며 연산이 빠르다
  - 딥러닝 : 머신러닝에서 진행하기 어려운 RNN, CNN 등 대량의 데이터를 처리하는데 아주 좋음

## 3. 비지도 학습

- 고차원 데이터를 저차원으로 줄이는 차원 축소
- 비슷한 데이터를 묶는 군집화

### (1) 데이터 전처리와 스케일 조정

- 스케일 조정 방법 : Standard Scaler, Robust Scaler, Min-MAX, Log

### (2) 차원축소, 특성 추출, 메니폴드 학습

#### 1) PCA 주성분 분석

- 데이터의 첫번째 방향과 직각인 방향 중에서 가장 많은 정보를 담은 방향을 찾음(고차원에는 이러한 직각이 무척 많음). 이를 주성분이라하고 고차원 데이터에서 이러한 주성분 데이터만 남김.
- 즉, 가장 유용한 방향을 찾아서 그 방향의 성분을 유지하는 것
- 정말 고차원일때 차원의 의미를 다소 뭉개서 학습 데이터로 사용은 가능하다. 예) 얼굴 인식
- PCA가 말하는 것: 데이터들을 정사영 시켜 차원을 낮춘다면, 어떤 벡터에 데이터들을 정사영 시켜야 원래의 데이터 구조를 제일 잘 유지할 수 있을까?

#### 2) NMF 비음수 행렬 분해

- PCA와 유사함. PCA에서는 데이터의 분산이 가장 크고 수직인 성분을 찾았다면, NMF는 음수가 아닌 성분과 계수 값을 찾는다.

#### 3) t-SNE를 이용한 매니폴드 학습

- 학습에 사용하지 않고 2차원으로 표현할 때 많이 사용한다.
- 데이터 포인트 사이의 거리를 가장 잘 보존하는 2차원 표현을 찾는 것.
- 멀리 떨어진 포인트와 거리를 보존하는 것보다 가까이 있는 포인트에 더 많은 비중을 두는 방식.
- 이를 통해 시각적으로 분류도 사실상 보이긴함

### (3) 군집

#### 1) K-means

- 데이터의 어떤 영역을 대표하는 클러스터 중심을 찾고, 데이터 포인트를 가장 가까운 중심에 할당하고, 그런 다음 클러스터에 할당된 데이터 포인트의 평균으로 클러스터 중심을 다시 지정.
- k-means는 모든 방향이 중요하다고 여긴다. 이때문에, 선형적인 클러스터, 원형이 클러스터는 잘 구분하지 못한다.
- 장단점
  - 장점 : 가볍다. 이해하기 쉽다.
  - 단점  : 선형, 원형적인 클러스터는 구분하기 어렵다. 즉, 복잡한 모형은 구분이 어려움

#### 2) 병합군집, 계층적 군집

#### 3) DBSCAN

- 특성 공간에서 가까이 있는 데이터가 많은 지역의 포인트를 찾음(밀집지역). 이 밀집 지역을 한 클러스터로 해서 비교적 비어있는 지역을 경계로 다른 클러스터와 구분함
- 한 데이터 포인트에서 특정 거리(eps로 거리 지정) 안에 데이터가 min_sample만큼 있으면 이를 핵심 샘플로 함.
- 장단점
  - 장점 : 노이즈 처리에 강함. 선형, 원형적인 클러스터도 가능
  - 단점 : 속도가 느리고, 거리와 min_sample 설정에 영향을 많이 받음

## 4. 데이터 표현, 특성공학

### (1) 데이터 특성 변경

- One-Hot Encoder
- 구간분할
  - 연속형 데이터를 특정 구간을 기준으로 나누어서 '여러 특성으로 만듬'
- 원본 특성에 기울기를 곱하는 방법
  - Linear Regression 사용
- log, exp, sin 같은 수학 함수를 적용해서 특성 변환 시도가능
  - log, exp : 데이터 스케일 변경
  - sin, cos : 주기적인 패턴 데이터에서 효과적
  - 이러한 데이터를 쓰는 이유는 대부분의 모델이 '정규분포와 비슷할 때 최고의 성능'을 낸다. 이런 모양을 만드려면 log, exp가 효과적

### (2) 특성 자동 선택

- 너무 많은 특성 속에서 적절한 특성을 선택하는 방법
- 일변량 통계(ANOVA), 모델 기반 선택, 반복적 선택
- ANOVA
  - 모델 학습 후 나타난 p값 기준으로만 판단함. 간단
- 모델 기반 특성
  - 중요도가 지정한 임계치(예를 들어 100개 특성중 50개)보다 큰 모든 특성을 선택
  - 특히, 랜덤포레스트를 사용하므로 복잡한 모델이지만 강력한 방법
- 반복적 특성 선택
  - 피쳐를 하나도 선택하지 않은 상태로 시작해서 하나씩 추가하면서 파악하는 방식. 또는 그 반대 방식(모든 피쳐 선택 후 하나씩 죽이는 방식)
- ★ 도메인 지식 사용

## 5. 모델 평가

### (1) 교차 검증 cross-validation

- k-겹 교차 검증으로 보통 5겹 또는 10겹을 사용
- 데이터를 비슷한 크기의 '부분 집합' 5개로 나눔. 첫번째 모델은 2~5 집합을 통해 학습, 두 번째 모델은 1, 3~5 집합을 통해 학습.
- 이를 통해 5개의 정확도를 평균내어 만듬
- 모델을 일반화할 수 있어서 좋음, 단 연산량 높음
- 비슷한 교차 검증
  - shuffle cross-validation
  - group cross-validation

### (2) 그리드 서치 grid search

- 모델의 하이퍼파라미터를 조정하는 것. 단순히 for문을 사용해서 중요한 파라미터를 변경시키면서 매번 모델 학습을 진행하는 것.

### (3) 평가지표

- 반드시 프로젝트의 최종 목표를 기준으로 작성해야하는 것임
- 종류 : 오차행렬, 정밀도, f-scroe, roc곡선